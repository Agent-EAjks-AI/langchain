# LangChain v1.0 message types

*Last updated: 08.08.25*

LangChain v0.4 allows developers to opt-in to new message types that will become default
in LangChain v1.0. LangChain v1.0 will be released this fall.

These messages should be considered a beta feature and are subject to change in
LangChain v1.0, although we do not anticipate any significant changes.

## Benefits

The new message types offer improvements in performance, type-safety, and consistency
across OpenAI, Anthropic, Gemini, and other providers.

### Performance

Importantly, the new messages are Python dataclasses, saving some runtime from
instantiating (layers of) Pydantic BaseModels.

LangChain v0.4 introduces a new `BaseChatModel` class in `langchain_core.v1.chat_models`
that is faster and leaner than the existing `BaseChatModel` class, offering significant
reductions in overhead above provider SDKs.

### Type-safety

Message content is typed as
```python
import langchain_core.messages.content_blocks as types

content: list[types.ContentBlock]
```

where we have introduced standard types for text, reasoning, citations, server-side
tool executions (e.g., web search and code interpreters). These include
[tool calls](https://python.langchain.com/docs/concepts/tool_calling/) and the
[multi-modal types](/docs/how_to/multimodal_inputs/) introduced in earlier versions
of LangChain. There are no breaking changes associated with the existing content types.

**This is the most significant change from the existing message classes**, which permit
strings, lists of strings, or lists of untyped dicts as content. We have added a
`.text` getter so that developers can easily recover string content. `.tool_calls`,
instead of an attribute, is now also a getter with an associated setter, so that
usage is largely the same. See [usage comparison](#usage-comparison), below, for details.

### Consistency

Many chat models can generate a variety of content in a single conversational turn,
including reasoning, tool calls and responses, images, text with citations, and other
structured objects. We have standardized these types, resulting in improved
inter-operability of messages across models.

## Usage comparison

| Task                    | Previous | New                                            |
|-------------------------|----------|------------------------------------------------|
| Get string content      | `message.content`            | `message.text`             |
| Get content blocks      | `message.content`            | `message.content`          |
| Get `additional_kwargs` | `message.additional_kwargs`  | `[block for block in message.content if block["type"] == "..."]` |
| Get `response_metadata` | `message.response_metadata`  | `message.response_metadata`|
| Get `tool_calls`        | `message.tool_calls`         | `message.tool_calls`       |

### Changes in content blocks

For providers that generate `list[dict]` content, the dict elements have changed to
conform to the new content block types. Refer to the
[API reference](https://python.langchain.com/api_reference/core/messages.html) for
details. Below we show some examples.

Importantly:
- Where provider-specific fields map to fields on standard types, LangChain manages
the translation.
- Where provider-specific fields do not map to fields on standard types, LangChain
stores them in an `"extras"` key (see below for examples).

#### Reasoning

<div style={{ display: "flex", gap: "1rem" }}>
  <div style={{ flex: 1 }}>
    **Before**
```python
from langchain.chat_models import init_chat_model

llm = init_chat_model(
    "openai:gpt-5",
    reasoning={"effort": "medium", "summary": "auto"},
    output_version="responses/v1",
)
response = llm.invoke(
    "What was the third tallest building in the world in the year 2000?"
)
response.content
```
```
[
  {
    "type": "reasoning",
    "id": "rs_abc123",
    "summary": [
      {
        "text": "The user is asking about...",
        "type": "summary_text"
      },
      {
        "text": "We should consider...",
        "type": "summary_text"
      }
    ]
  },
  {
    "type": "text",
    "text": "In the year 2000 the third-tallest building in the world was...",
    "id": "msg_abc123"
  }
]
```
  </div>

  <div style={{ flex: 1 }}>
    **After**
```python
from langchain.chat_models import init_chat_model

llm = init_chat_model(
    "openai:gpt-5",
    reasoning={"effort": "medium", "summary": "auto"},
    message_version="v1",
)
response = llm.invoke(
    "What was the third tallest building in the world in the year 2000?"
)
response.content
```
```
[
  {
    "type": "reasoning",
    "reasoning": "The user is asking about...",
    "id": "rs_abc123"
  },
  {
    "type": "reasoning",
    "reasoning": "We should consider...",
    "id": "rs_abc123"
  },
  {
    "type": "text",
    "text": "In the year 2000 the third-tallest building in the world was...",
    "id": "msg_abc123"
  }
]
```
  </div>
</div>

<details Before>
```python
from langchain.chat_models import init_chat_model

llm = init_chat_model(
    "openai:gpt-5",
    reasoning={"effort": "medium", "summary": "auto"},
    output_version="responses/v1",
)
response = llm.invoke(
    "What was the third tallest building in the world in the year 2000?"
)
response.content
```
```
[
  {
    "type": "reasoning",
    "id": "rs_abc123",
    "summary": [
      {
        "text": "The user is asking about...",
        "type": "summary_text"
      },
      {
        "text": "We should consider...",
        "type": "summary_text"
      }
    ]
  },
  {
    "type": "text",
    "text": "In the year 2000 the third-tallest building in the world was...",
    "id": "msg_abc123"
  }
]
```
</details>

<details After>
```python
from langchain.chat_models import init_chat_model

llm = init_chat_model(
    "openai:gpt-5",
    reasoning={"effort": "medium", "summary": "auto"},
    message_version="v1",
)
response = llm.invoke(
    "What was the third tallest building in the world in the year 2000?"
)
response.content
```
```
[
  {
    "type": "reasoning",
    "reasoning": "The user is asking about...",
    "id": "rs_abc123"
  },
  {
    "type": "reasoning",
    "reasoning": "We should consider...",
    "id": "rs_abc123"
  },
  {
    "type": "text",
    "text": "In the year 2000 the third-tallest building in the world was...",
    "id": "msg_abc123"
  }
]
```
</details>

#### Citations and web search

<details Before>
```python
from langchain.chat_models import init_chat_model

llm = init_chat_model("openai:gpt-5-mini", output_version="responses/v1")
llm_with_tools = llm.bind_tools([{"type": "web_search_preview"}])

response = llm_with_tools.invoke("What was a positive news story from today?")
response.content
```
```
[
  {
    "type": "reasoning",
    "id": "rs_abc123",
    "summary": []
  },
  {
    "type": "web_search_call",
    "id": "ws_abc123",
    "action": {
      "query": "positive news today August 8 2025 'good news' 'Aug 8 2025' 'today' ",
      "type": "search"
    },
    "status": "completed"
  },
  {
    "type": "text",
    "text": "Here are two positive news items from today...",
    "annotations": [
      {
        "type": "url_citation",
        "end_index": 455,
        "start_index": 196,
        "title": "Document title",
        "url": "<document url>"
      },
      {
        "type": "url_citation",
        "end_index": 1022,
        "start_index": 707,
        "title": "Another Document",
        "url": "<another document url>"
      },
    ],
    "id": "msg_abc123"
  }
]
```
</details>

<details After>
```python
from langchain.chat_models import init_chat_model

llm = init_chat_model("openai:gpt-5-mini", message_version="v1")
llm_with_tools = llm.bind_tools([{"type": "web_search_preview"}])

response = llm_with_tools.invoke("What was a positive news story from today?")
response.content
```
```
[
  {
    "type": "reasoning",
    "id": "rs_abc123"
  },
  {
    "type": "web_search_call",
    "id": "ws_abc123",
    "query": "positive news August 8 2025 'good news' 'today' ",
    "extras": {
      "action": {"type": "search"},
      "status": "completed",
    }
  },
  {
    "type": "web_search_result",
    "id": "ws_abc123"
  },
  {
    "type": "text",
    "text": "Here are two positive news items from today...",
    "annotations": [
      {
        "type": "citation",
        "end_index": 455,
        "start_index": 196,
        "title": "Document title",
        "url": "<document url>"
      },
      {
        "type": "citation",
        "end_index": 1022,
        "start_index": 707,
        "title": "Another Document",
        "url": "<another document url>"
      }
    ],
    "id": "msg_abc123"
  }
]
```
</details>

#### Non-standard blocks

Where content blocks from specific providers do not map to a standard type, they are
structured into a `"non_standard"` block:
```python
{
    "type": "non_standard",
    "value": original_block,
}
```
<details Before>
...
</details>


## Feature gaps

The new message types do not yet support LangChain's caching layer. Support will be
added in the coming weeks.
